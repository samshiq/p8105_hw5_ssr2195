---
title: "Homework 5"
author: "Samina Rashiq"
date: "`r Sys.Date()`"
output: github_document
---

```{r setup, include = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(broom)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1
```{r problem_1}

#Set seed for reproducibility
set.seed(4200)

# Create birthday simulation function
bday_sim = function(n) {
  bdays = sample(1:365, size = n, replace = TRUE)
  duplicate = length(unique(bdays)) < n
  return(duplicate)
}

# Run simulation from for group sizes between 2-50 10000 times
sim_res = 
  expand_grid(
    n = 2:50,
    iter = 1:10000
  ) |> 
  mutate(res = map_lgl(n, bday_sim)) |> 
  group_by(n) |> 
  summarize(prob = mean(res))

# Plot probability as a function of group size
sim_res |> 
  ggplot(aes(x = n, y = prob )) + 
  geom_line()
```


As group size increases, the probability that two people in the group share the same birthday increases as well. 

## Problem 2
```{r}

# Set seed for reproducibility
set.seed(1738)

# Set design elements
n <- 30       # Sample size
std_dev <- 5  # Standard deviation
hypotheses <- c(0, 1, 2, 3, 4, 5, 6)  # Hypothesized means

# Generate datasets for each true mean
datasets <- expand_grid(
  dataset_id = 1:5000,
  true_mean = hypotheses
) %>%
  mutate(data = map(true_mean, ~ rnorm(n, mean = .x, sd = std_dev)))

# Perform t-tests and compute sample means and p-values
t_test_results <- datasets %>%
  mutate(
    mu_hat = map_dbl(data, mean),                            
    p_value = map_dbl(data, ~ t.test(.x, mu = 0)$p.value)
  )

```

```{r}
# Calculate power for each true mean
power_results <- t_test_results %>%
  group_by(true_mean) %>%
  summarize(power = mean(p_value < 0.05), .groups = "drop")  # Proportion of rejections

# Plot power versus true mean
plot_power <- ggplot(power_results, aes(x = true_mean, y = power)) +
  geom_line() +
  geom_point() +
  labs(
    title = "Power of the Test vs. True Mean",
    x = "True Mean (mu)",
    y = "Power (Proportion of Null Rejections)"
  ) +
  theme_minimal()

print(plot_power)

```

```{r}
# Calculate average mu_hat for all datasets
average_mu_results <- t_test_results %>%
  group_by(true_mean) %>%
  summarize(avg_mu_hat = mean(mu_hat), .groups = "drop")

# Calculate average mu_hat for rejected samples only
average_mu_rejected <- t_test_results %>%
  filter(p_value < 0.05) %>%
  group_by(true_mean) %>%
  summarize(avg_mu_rejected = mean(mu_hat), .groups = "drop")

# Combine overall and rejected averages for combined plot
combined_results <- average_mu_results %>%
  left_join(average_mu_rejected, by = "true_mean")



# Plot average mu_hat vs true mean
plot_avg_mu <- ggplot(average_mu_results, aes(x = true_mean, y = avg_mu_hat)) +
  geom_line(color = "deeppink", size = 1) +
  geom_point(color = "deeppink", size = 2) +
  labs(
    title = "Average Estimate of Mu vs. True Mean",
    x = "True Mean (mu)",
    y = "Average Estimate of Mu"
  ) +
  theme_minimal()

print(plot_avg_mu)


# Plot overall average vs rejected sample average
plot_combined <- ggplot(combined_results, aes(x = true_mean)) +
  geom_line(aes(y = avg_mu_hat, color = "Overall Average"), size = 1) +
  geom_point(aes(y = avg_mu_hat, color = "Overall Average")) +
  geom_line(aes(y = avg_mu_rejected, color = "Rejected Samples"), size = 1) +
  geom_point(aes(y = avg_mu_rejected, color = "Rejected Samples")) +
  scale_color_manual(values = c("Overall Average" = "deeppink", "Rejected Samples" = "hotpink")) +
  labs(
    title = "Average Estimate of Mu vs True Mean",
    x = "True Mean (mu)",
    y = "Average Estimate of Mu",
    color = "Legend"
  ) +
  theme_minimal()

print(plot_combined)

```


No, the sample average of mu-hat from rejected tests is not approximately equal to the true value of mu, especially when mu is small. This is because we only reject the null hypothesis when the estimate mu-hat is more extreme, which leads to a higher average for rejected samples.

However, as mu gets larger, the rejected-samples average (blue line) approaches the true value (black line) because we are rejecting the null hypothesis more frequently. This means weâ€™re including nearly all samples, not just the extreme ones, which reduces the bias.



## Problem 3
```{r }

homicide_dta <- 
  read_csv("homicide-data.csv") 


# Create city_state variable and convert reported_date
homicide_dta <- homicide_dta |>
  mutate(city_state = str_c(city, ", ", state)) |>
  mutate(reported_date = as.character(reported_date)) |>
  mutate(reported_date = as.Date(reported_date, format = "%Y%m%d"))

# Summarize homicides
summary <- homicide_dta |>
  group_by(city_state) |>
  summarise(
    total_homicides = n(),
    unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest")),
    solved_homicides = sum(disposition == "Closed by arrest")
  )

# View summary
print(summary)


```

This publicly-available dataset from the Washington Post contains homicide
data from cities in the US. The dataset contains `r nrow(homicide_dta)` homicide cases and has `r ncol(homicide_dta)` columns, with variables including victim name, age, race, sex, and disposition of the case. It contains data from `r homicide_dta %>% pull(city_state) %>% unique() %>% length()` cities.

```{r}


# Filter for Baltimore cases
baltimore <- homicide_dta |> 
  filter(city_state == "Baltimore, MD")

# Count number unsolved/total homicides
total_cases <- nrow(baltimore)
unsolved_cases <- baltimore |> 
  filter(disposition == "Open/No arrest") |>  
  nrow()

# Perform the prop test
baltimore_prop_test <- prop.test(
  x = unsolved_cases, 
  n = total_cases,
  alternative = "two.sided"
)

# Tidy result and extract proportion and CI's
baltimore_prop_test_summary <- baltimore_prop_test |> 
  tidy() |> 
  select(estimate, conf.low, conf.high)

print(baltimore_prop_test_summary)


```

```{r}

# Reassign Tulsa, AL (does not exist) to Tulsa, OK
homicide_dta <- homicide_dta |> 
  mutate(
    city_state = case_when(
      city_state == "Tulsa, AL" ~ "Tulsa, OK", 
      TRUE ~ city_state
    )
  )

# Summarize total/unsolved cases by city
city_data <- homicide_dta |> 
  group_by(city_state) |> 
  summarise(
    total_cases = n(),
    unsolved_cases = sum(disposition == "Open/No arrest"),
    .groups = "drop"
  )

# Run prop.test for each city
city_results <- city_data |> 
  mutate(
    prop_test_result = map2(
      unsolved_cases, 
      total_cases, 
      ~ prop.test(x = .x, n = .y, alternative = "two.sided")
    ),
    tidy_result = map(prop_test_result, tidy)
  ) |> 
  unnest(tidy_result) |>
  select(city_state, estimate, conf.low, conf.high)

# View results
city_results

```

```{r}
library(ggplot2)

# Plot the results
city_results |> 
  mutate(city_state = reorder(city_state, estimate)) |>
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +
  coord_flip() +
  labs(
    title = "Proportion of Unsolved Homicides by City",
    x = "City",
    y = "Proportion of Unsolved Homicides (95% CI)",
  ) +
  theme_minimal() +
  theme(
    axis.text.y = element_text(size = 6)  # Adjust text size for city labels
  )

```






